{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8374,
     "status": "ok",
     "timestamp": 1761672645128,
     "user": {
      "displayName": "verewolf323",
      "userId": "10852833045107436259"
     },
     "user_tz": 0
    },
    "id": "PWx0AafN0X27",
    "outputId": "931cbccd-78a2-4018-ff90-a834862deaa9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
      "Requirement already satisfied: more_itertools in /usr/local/lib/python3.12/dist-packages (10.8.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
      "Collecting bioc\n",
      "  Downloading bioc-2.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
      "Requirement already satisfied: lxml>=4.6.3 in /usr/local/lib/python3.12/dist-packages (from bioc) (5.4.0)\n",
      "Collecting jsonlines>=1.2.0 (from bioc)\n",
      "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting intervaltree (from bioc)\n",
      "  Downloading intervaltree-3.1.0.tar.gz (32 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting docopt (from bioc)\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonlines>=1.2.0->bioc) (25.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: sortedcontainers<3.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from intervaltree->bioc) (2.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n",
      "Downloading bioc-2.1-py3-none-any.whl (33 kB)\n",
      "Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
      "Building wheels for collected packages: docopt, intervaltree\n",
      "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=fb15323952ac2b3d42cec29cbb9ec06881c51039803e71e1802f58344ca617a5\n",
      "  Stored in directory: /root/.cache/pip/wheels/1a/bf/a1/4cee4f7678c68c5875ca89eaccf460593539805c3906722228\n",
      "  Building wheel for intervaltree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for intervaltree: filename=intervaltree-3.1.0-py2.py3-none-any.whl size=26098 sha256=6e4959b6242652a86ec2a7167cd6a332e4af9e1496e27744ad793e9d2b4747b8\n",
      "  Stored in directory: /root/.cache/pip/wheels/65/c3/c3/238bf93c243597857edd94ddb0577faa74a8e16e9585896e83\n",
      "Successfully built docopt intervaltree\n",
      "Installing collected packages: docopt, jsonlines, intervaltree, bioc\n",
      "Successfully installed bioc-2.1 docopt-0.6.2 intervaltree-3.1.0 jsonlines-4.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers tqdm more_itertools scikit-learn torch bioc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mw21duT03EV-"
   },
   "outputs": [],
   "source": [
    "# utils file\n",
    "\n",
    "import gzip\n",
    "import random\n",
    "import bioc\n",
    "import json\n",
    "\n",
    "# apparently .gz and .tar.gz are different things\n",
    "# this is a tar archive\n",
    "# source = \"example_bioc_files.tar.gz\"\n",
    "# this is a gzip-compressed (.gz) single file\n",
    "# BC5CDR = \"processed_sources/bc5cdr_train.bioc.xml.gz\"  # the source path is determined by pwd\n",
    "\n",
    "\n",
    "def extract_first_n_docs(path: str, n: int | None = None):\n",
    "    \"\"\"\n",
    "    Read a gzipped BioC XML file at `path` and return a list with info for the first `n` documents.\n",
    "    If n is None (default), return info for all documents.\n",
    "    Each list item is a dict: {'id': ..., 'passages': [ {'text': ..., 'annotations': [ {...}, ... ]}, ... ] }\n",
    "    \"\"\"\n",
    "    if n is not None and n < 0:\n",
    "        raise ValueError(\"n must be non-negative\")\n",
    "\n",
    "    results = []\n",
    "    with gzip.open(path, 'rt', encoding='utf-8') as file:\n",
    "        data = file.read()\n",
    "\n",
    "    collection = bioc.biocxml.loads(data)\n",
    "    docs = collection.documents if n is None else collection.documents[:n]\n",
    "    for document in docs:\n",
    "        doc_info = {'id': document.id, 'passages': []}\n",
    "        for passage in document.passages:\n",
    "            passage_info = {'text': passage.text, 'annotations': []}\n",
    "            for anno in passage.annotations:\n",
    "                # serialize common fields of a BioC annotation\n",
    "                anno_info = {\n",
    "                    'id': getattr(anno, 'id', None),\n",
    "                    'text': getattr(anno, 'text', None),\n",
    "                    'infons': dict(getattr(anno, 'infons', {})),\n",
    "                    'locations': [\n",
    "                        {'offset': getattr(loc, 'offset', None), 'length': getattr(loc, 'length', None)}\n",
    "                        for loc in getattr(anno, 'locations', [])\n",
    "                    ]\n",
    "                }\n",
    "                passage_info['annotations'].append(anno_info)\n",
    "            doc_info['passages'].append(passage_info)\n",
    "        results.append(doc_info)\n",
    "\n",
    "    return results\n",
    "\n",
    "def get_mention_names_id_pairs(path: str):\n",
    "    docs = extract_first_n_docs(path)\n",
    "    name_id_pairs = []\n",
    "    for doc in docs:\n",
    "        for passage in doc['passages']:\n",
    "            for anno in passage['annotations']:\n",
    "                name_id_pairs.append((anno['text'], anno['infons'].get('concept_id')))\n",
    "    return name_id_pairs\n",
    "\n",
    "\n",
    "# mesh = \"processed_sources/mesh2015.json.gz\"\n",
    "\n",
    "def read_first_n_from_json_gz(path: str, n: int | None = None) -> list:\n",
    "    \"\"\"\n",
    "    Read a gzipped JSON file at `path` and return the first `n` elements if the top-level\n",
    "    JSON value is a list. If n is None (default), return all entries.\n",
    "    Raises ValueError for non-list top-level or invalid `n`.\n",
    "    \"\"\"\n",
    "    if n is not None and n < 0:\n",
    "        raise ValueError(\"n must be non-negative\")\n",
    "\n",
    "    with gzip.open(path, 'rt', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    if not isinstance(data, list):\n",
    "        raise ValueError(f\"JSON top-level is {type(data).__name__}, expected list\")\n",
    "\n",
    "    return data if n is None else data[:n]\n",
    "\n",
    "\n",
    "def get_entity_name_id_pairs(path: str):\n",
    "    entities = read_first_n_from_json_gz(path)\n",
    "    name_id_pairs = [(entity['name'], entity['id']) for entity in entities]\n",
    "    return name_id_pairs\n",
    "\n",
    "\n",
    "# function to get an entry by id\n",
    "def get_entry_by_id(data: list, entry_id: str) -> dict | None:\n",
    "    \"\"\"\n",
    "    Get a JSON entry by its ID from a list of entries.\n",
    "    \"\"\"\n",
    "    for item in data:\n",
    "        if item.get(\"id\") == entry_id:\n",
    "            return item\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0cGeqCz07Rzn"
   },
   "outputs": [],
   "source": [
    "# There are 2 inputs to the model which I need to embed and then compare:\n",
    "# 1. The mention entity name\n",
    "# 2. The name of all entities\n",
    "\n",
    "# functions:\n",
    "# function to extract the name from every mention\n",
    "# function to extract the name from the mesh entry\n",
    "\n",
    "# import model\n",
    "# prepare both types of inputs\n",
    "# construct positives and negatives 1:4 ratio\n",
    "# train on the data constructed so that dot product/ cosine similarity is high when the entity is correctly matched and low when no match\n",
    "\n",
    "\n",
    "from utils import *\n",
    "\n",
    "# BC5CDR input  (mention names and ids)\n",
    "bc5cdr_name_id_pairs = get_mention_names_id_pairs(\"processed_sources/bc5cdr_train.bioc.xml.gz\")\n",
    "\n",
    "\n",
    "# MeSH2015 input (entity names and ids)\n",
    "mesh_name_id_pairs = get_entity_name_id_pairs(\"processed_sources/mesh2015.json.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1761674201492,
     "user": {
      "displayName": "verewolf323",
      "userId": "10852833045107436259"
     },
     "user_tz": 0
    },
    "id": "jEZLMkRa8_uM",
    "outputId": "eb6de2d6-5f3d-4b24-a88c-514326c82f7e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Naloxone', 'MESH:D009270'),\n",
       " ('clonidine', 'MESH:D003000'),\n",
       " ('hypertensive', 'MESH:D006973'),\n",
       " ('clonidine', 'MESH:D003000'),\n",
       " ('hypotensive', 'MESH:D007022')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bc5cdr_name_id_pairs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1761674203733,
     "user": {
      "displayName": "verewolf323",
      "userId": "10852833045107436259"
     },
     "user_tz": 0
    },
    "id": "KAXCzjMX9ETU",
    "outputId": "1d7ed411-f7a5-4aa3-8341-39dbd04f219d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Calcimycin', 'MESH:D000001'),\n",
       " ('Temefos', 'MESH:D000002'),\n",
       " ('Abattoirs', 'MESH:D000003'),\n",
       " ('Abbreviations as Topic', 'MESH:D000004'),\n",
       " ('Abdomen', 'MESH:D000005')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mesh_name_id_pairs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gHiC6Da_7UFF"
   },
   "outputs": [],
   "source": [
    "# constructing positive pairs\n",
    "# take all mention names and get the corresponding entity name by matching the id\n",
    "\n",
    "mesh_id_to_name = {entity_id: entity_name for entity_name, entity_id in mesh_name_id_pairs}\n",
    "\n",
    "positive_pairs = [\n",
    "    (mention_name, mesh_id_to_name[mention_id])\n",
    "    for mention_name, mention_id in bc5cdr_name_id_pairs\n",
    "    if mention_id in mesh_id_to_name\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1761674211792,
     "user": {
      "displayName": "verewolf323",
      "userId": "10852833045107436259"
     },
     "user_tz": 0
    },
    "id": "p7PgDbhh7xwl",
    "outputId": "db843ead-0a0b-4b1d-c268-4c213c828f86"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Naloxone', 'Naloxone'),\n",
       " ('clonidine', 'Clonidine'),\n",
       " ('hypertensive', 'Hypertension'),\n",
       " ('clonidine', 'Clonidine'),\n",
       " ('hypotensive', 'Hypotension')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_pairs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xm4qrrlZ7WBw"
   },
   "outputs": [],
   "source": [
    "# constructing negative pairs\n",
    "# I want to have 4 times as many negative pairs as positive pairs\n",
    "# There are several negative sampling techinques\n",
    "# for now take one mention name from the positives and one entity name from the positives that don't match\n",
    "# make it sample randomly from the positives\n",
    "import random\n",
    "negative_pairs = []\n",
    "while len(negative_pairs) < 4 * len(positive_pairs):\n",
    "    mention_name, mention_id = random.choice(positive_pairs)\n",
    "    entity_name, entity_id = random.choice(positive_pairs)\n",
    "    if mention_id != entity_id:\n",
    "        negative_pairs.append((mention_name, entity_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1761674220310,
     "user": {
      "displayName": "verewolf323",
      "userId": "10852833045107436259"
     },
     "user_tz": 0
    },
    "id": "2iM5V6qd71nk",
    "outputId": "3bcd5776-53d7-4c7b-f422-798277f5b0ad"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ESRD', 'vitamin K'),\n",
       " ('cardiomyopathy', 'N-pyrimidinyl-2-phenoxyacetamide'),\n",
       " ('osteopenia', 'renal dysfunction'),\n",
       " ('Dopamine', 'appetite suppressants'),\n",
       " ('estradiol', 'cognitive impairment')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_pairs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QLWAlLyG7Yh0"
   },
   "outputs": [],
   "source": [
    "# We'll turn them from tuples into dictionaries with boolean label of whether they are positive or negative pairs:\n",
    "training_data = []\n",
    "for mention_name, entity_name in positive_pairs:\n",
    "    training_data.append({'mention_name': mention_name, 'entity_name': entity_name, 'label': True})\n",
    "for mention_name, entity_name in negative_pairs:\n",
    "    training_data.append({'mention_name': mention_name, 'entity_name': entity_name, 'label': False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "23LNAFdB7dL_"
   },
   "outputs": [],
   "source": [
    "# Now we'll split our dataset into training, validation and test splits:\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_pairs, valtest_pairs = train_test_split(training_data, train_size=0.6, random_state=43)\n",
    "val_pairs, test_pairs = train_test_split(valtest_pairs, train_size=0.5, random_state=43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1761674436510,
     "user": {
      "displayName": "verewolf323",
      "userId": "10852833045107436259"
     },
     "user_tz": 0
    },
    "id": "HfmZArXz-pKk",
    "outputId": "fa7f6790-dfb4-4153-8c91-4942099a5e12"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'mention_name': '3alpha-hydroxy-3beta-methyl-5alpha-pregnan-20-one',\n",
       "  'entity_name': 'propofol',\n",
       "  'label': False},\n",
       " {'mention_name': 'catecholamine',\n",
       "  'entity_name': 'Catecholamines',\n",
       "  'label': True},\n",
       " {'mention_name': 'Heparan sulphate',\n",
       "  'entity_name': 'phenylephrine',\n",
       "  'label': False},\n",
       " {'mention_name': 'AMI', 'entity_name': 'Lithium', 'label': False},\n",
       " {'mention_name': 'dipyridamole',\n",
       "  'entity_name': 'retention of urine',\n",
       "  'label': False},\n",
       " {'mention_name': 'iopamidol', 'entity_name': 'Fentanyl', 'label': False},\n",
       " {'mention_name': 'nephropathy',\n",
       "  'entity_name': 'streptozotocin',\n",
       "  'label': False},\n",
       " {'mention_name': 'thrombosis', 'entity_name': 'nephrotoxic', 'label': False}]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = train_pairs[:8]\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 263,
     "referenced_widgets": [
      "a0dfd0843e0940da98a9fc4c8633ed40",
      "7655f2f67cb7465299930de0668401e9",
      "9549c2df779f49f6bd2a822c0784c1b4",
      "0c85c112b8e34a34a89943408589247d",
      "81810b9a23044e858630645485f61620",
      "200a6d47045245a4a0bddf4b594266ef",
      "2606a8f281e5479eb593ea3d995ee6b4",
      "887032c5a4834c938567683d158c7793",
      "35d1c451c6744c3285a03c7995a24f23",
      "9272ed72b8a245a5bd3d635b5d613fae",
      "fc1e340313924ac0b148573d61965541",
      "61828d7d6a564e1185ef50c3c8426f6a",
      "50c7485a333e4eb68d0bd78d3d9436da",
      "48f35867f5d94992be420163523b3964",
      "d2fea0a9dfda4af290e084e88a271b01",
      "ed63733b0fe741c3b5e2f9ef03da722b",
      "1863f7cd6ce54d039c6f6f90b1789c49",
      "c21eea44a6ec43fdaaff58e68dd2087f",
      "aeaad98965704e04a5f331332cd30e67",
      "68aaa927880e4debaf77685d394166f5",
      "d0730129101c4ac285e26c7b00a38945",
      "e0fdf70e23ea401e9758b35bffc5fda9",
      "c0618a94b6454b89be660fc33c7166a1",
      "0f69b223fb884a248966ddc58bb76f19",
      "da22035c1ab54b68aa2131bd06bc9d09",
      "1e0ec4e429d24befa5f52fb1892317c1",
      "e9202ca29cc34051acb425d6a1b3d8c2",
      "1a40a22b44d64e3c82fd6dc5e38d49b6",
      "f7c836970b32445c9dd2c507717c894b",
      "d36f1f8602724c6bac2231b99a65f79a",
      "dc2cf4615ef84ac5b63fae20afeedeee",
      "c0c570136a5e455899217a160381c4ba",
      "51f9c83e345245a18b5bd7a6246f03ca",
      "1df6fb95aee348569a8b8e524316ba4c",
      "cdaab9381ff44145ae182d364f32b939",
      "46a489548573442883cfde7de57448a2",
      "c2754c04088b410ab251473a2c6518bb",
      "4a3838669eaa4f2f97719c08fa7851dc",
      "e8aa0cbc54b9428d941e27a05b09951c",
      "5fc2e1424b0c40f3b7b41de83de4e4a7",
      "fa1c40fbaa3f4073a805b65240e58c58",
      "1a112b20e7194443949be68d26b9d6d8",
      "6313f817c785497bbd7f208d2e668ed2",
      "9e69bea26fef46b9b09d4dc995a96b55",
      "a13878ddacb84165a8c6ab3576321193",
      "5848476aea49412b982f25334d20a6f7",
      "c0ef9ae20dd649a49a5e6cbbfa71d6f2",
      "6484693775fc4cfa89cf19e63006470c",
      "b32e9bed17bb4987b2b0d56dbf4a4a41",
      "71fb021b21604026b808bb8d0f41a1f3",
      "4b908b298ef44945b70ce9866cb4ca6b",
      "41fdbc9bcdf3488792f4178988cba2ec",
      "461084ecc45a4899882082d8b7689fd2",
      "5133824ca8024afd8dd1a6efeabea4b9",
      "ff6e602b65cc47b4a1e3b81192d63e6b",
      "239813d01c99457a924a429a373e5265",
      "62aafb235e1d4de388154e4f3fd9b684",
      "fa11df05ac18438ca86040e35d4e9058",
      "0391cd101b734c1d9b54f55a7bb1f66c",
      "22f163c6f75640d681cabcb2e89d3648",
      "1ac2de748152418d9baabf6c7d827b9c",
      "1c59162b97dc42629437444d6a431d31",
      "8780e39bd1b84080947b57e0ef0010d4",
      "1c3fbb7dbba74f92b733d196dfd0e4b2",
      "c20a14438e9241c2871641e2f20b7008",
      "1a7f1cec43f74d6d9b6c00c27f616a3a"
     ]
    },
    "executionInfo": {
     "elapsed": 1034247,
     "status": "ok",
     "timestamp": 1761675559166,
     "user": {
      "displayName": "verewolf323",
      "userId": "10852833045107436259"
     },
     "user_tz": 0
    },
    "id": "mNdzzuZW2C6u",
    "outputId": "9cbf052b-75a4-4f6a-a240-667cf992afcf"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0dfd0843e0940da98a9fc4c8633ed40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3497 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61828d7d6a564e1185ef50c3c8426f6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1166 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0 train_loss=5.8265 train_f1=0.4979 val_loss=5.3866 val_f1=0.5205\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0618a94b6454b89be660fc33c7166a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3497 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1df6fb95aee348569a8b8e524316ba4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1166 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=1 train_loss=3.3259 train_f1=0.5555 val_loss=5.1988 val_f1=0.5279\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a13878ddacb84165a8c6ab3576321193",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3497 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "239813d01c99457a924a429a373e5265",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1166 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=2 train_loss=2.3499 train_f1=0.6035 val_loss=5.0622 val_f1=0.5302\n"
     ]
    }
   ],
   "source": [
    "# This loads the model directly which allows fine-tuning\n",
    "# All the below is for fine-tuning the base SapBERT model\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "model_name = \"cambridgeltl/SapBERT-from-PubMedBERT-fulltext\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# define loss function\n",
    "import torch\n",
    "loss_func = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "# define training parameters\n",
    "batch_size = 8\n",
    "num_epochs = 3\n",
    "learning_rate = 1e-5\n",
    "\n",
    "# define optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# for jupyter notebook\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "# training loop\n",
    "from more_itertools import chunked\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "  model.train()\n",
    "  train_loss = 0\n",
    "  random.shuffle(train_pairs)\n",
    "  train_batches = list(chunked(train_pairs, batch_size))\n",
    "  train_predictions, train_labels = [], []\n",
    "\n",
    "  for batch in tqdm(train_batches):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    tokenized1 = tokenizer( [ x['mention_name'] for x in batch ], max_length=512, padding=True, truncation=True, return_tensors='pt' )\n",
    "    outputs1 = model( input_ids=tokenized1['input_ids'].to(device), attention_mask=tokenized1['attention_mask'].to(device) )\n",
    "    cls_vectors1 = outputs1.last_hidden_state[:,0,:]\n",
    "\n",
    "    tokenized2 = tokenizer( [ x['entity_name'] for x in batch ], max_length=512, padding=True, truncation=True, return_tensors='pt' )\n",
    "    outputs2 = model( input_ids=tokenized2['input_ids'].to(device), attention_mask=tokenized2['attention_mask'].to(device) )\n",
    "    cls_vectors2 = outputs2.last_hidden_state[:,0,:]\n",
    "\n",
    "    dotproducts = (cls_vectors1 * cls_vectors2).sum(axis=1)\n",
    "\n",
    "    labels = torch.tensor([ float(x['label']) for x in batch ]).to(device)\n",
    "\n",
    "    loss = loss_func(dotproducts, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    train_loss += loss.item()\n",
    "\n",
    "    train_predictions += [ dotproduct > 0 for dotproduct in dotproducts.cpu().tolist() ]\n",
    "    train_labels += [ x['label'] for x in batch ]\n",
    "\n",
    "  model.eval()\n",
    "  val_loss = 0\n",
    "  val_batches = list(chunked(val_pairs, batch_size))\n",
    "  val_predictions, val_labels = [], []\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for batch in tqdm(val_batches):\n",
    "\n",
    "      tokenized1 = tokenizer( [ x['mention_name'] for x in batch ], max_length=512, padding=True, truncation=True, return_tensors='pt' )\n",
    "      outputs1 = model( input_ids=tokenized1['input_ids'].to(device), attention_mask=tokenized1['attention_mask'].to(device) )\n",
    "      cls_vectors1 = outputs1.last_hidden_state[:,0,:]\n",
    "\n",
    "      tokenized2 = tokenizer( [ x['entity_name'] for x in batch ], max_length=512, padding=True, truncation=True, return_tensors='pt' )\n",
    "      outputs2 = model( input_ids=tokenized2['input_ids'].to(device), attention_mask=tokenized2['attention_mask'].to(device) )\n",
    "      cls_vectors2 = outputs2.last_hidden_state[:,0,:]\n",
    "\n",
    "      dotproducts = (cls_vectors1 * cls_vectors2).sum(axis=1)\n",
    "\n",
    "      labels = torch.tensor([ float(x['label']) for x in batch ]).to(device)\n",
    "\n",
    "      loss = loss_func(dotproducts, labels)\n",
    "\n",
    "      val_loss += loss.item()\n",
    "\n",
    "      # training it to predict positive if dotproduct > 0\n",
    "      val_predictions += [ dotproduct > 0 for dotproduct in dotproducts.cpu().tolist() ]\n",
    "      val_labels += [ x['label'] for x in batch ]\n",
    "\n",
    "  train_loss /= len(train_batches)\n",
    "  val_loss /= len(val_batches)\n",
    "\n",
    "  train_f1 = f1_score(train_labels, train_predictions, zero_division=0)\n",
    "  val_f1 = f1_score(val_labels, val_predictions, zero_division=0)\n",
    "\n",
    "  print(f\"{epoch=} {train_loss=:.4f} {train_f1=:.4f} {val_loss=:.4f} {val_f1=:.4f}\")\n",
    "\n",
    "\n",
    "model.save_pretrained(\"model/name_only_entity_linking_finetuned_model\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMqox/DVuRuI4JSsPKrHR3X",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}