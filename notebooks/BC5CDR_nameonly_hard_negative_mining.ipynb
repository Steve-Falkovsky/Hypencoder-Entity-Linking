{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Steve-Falkovsky/Hypencoder-Entity-Linking/blob/main/notebooks/BC5CDR_nameonly_hard_negative_mining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6fWDYcI90JIV"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import importlib.util\n",
    "\n",
    "REPO_NAME = \"Hypencoder-Entity-Linking\"\n",
    "GIT_URL = f\"https://github.com/Steve-Falkovsky/{REPO_NAME}.git\"\n",
    "BRANCH_NAME = \"main\"\n",
    "\n",
    "if not os.path.exists(REPO_NAME):\n",
    "    !git clone -b {BRANCH_NAME} --single-branch {GIT_URL}\n",
    "\n",
    "    # Move into the downloaded repo (The Root)\n",
    "    os.chdir(REPO_NAME)\n",
    "\n",
    "\n",
    "%pip install -q -e \"./hypencoder-paper\"\n",
    "\n",
    "os.chdir(\"hypencoder-paper\")\n",
    "\n",
    "print(f\"üìç Working Directory is now: {os.getcwd()}\")\n",
    "print(\"‚úÖ Environment Ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IW5f-nK3rERc"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# there are all \"positive\" pairs\"\n",
    "dataset = load_dataset(\"Stevenf232/BC5CDR_MeSH2015_nameonly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CF8-okEYP5m_"
   },
   "source": [
    "### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wN0mMKMd80bH"
   },
   "outputs": [],
   "source": [
    "# Core Hypencoder model for outputing dense vector representations\n",
    "from hypencoder_cb.modeling.hypencoder import Hypencoder, HypencoderDualEncoder, TextEncoder\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"Stevenf232/SapBERT_freeze_hypencoder\"\n",
    "\n",
    "dual_encoder = HypencoderDualEncoder.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "query_encoder: Hypencoder = dual_encoder.query_encoder\n",
    "passage_encoder: TextEncoder = dual_encoder.passage_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "spUPv5HAQBip"
   },
   "source": [
    "### Move the model to the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LYjL82bYPt5r"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Setup the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")  # This should say 'cuda'\n",
    "\n",
    "# Move the model to the GPU\n",
    "passage_encoder.to(device)\n",
    "query_encoder.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dLFqpGzvQQj_"
   },
   "source": [
    "### Load datasets and tokenise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qSxyVkRPBkCd"
   },
   "source": [
    "# Passage Encodings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wiCD611G0cu8"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.amp import autocast\n",
    "\n",
    "def batch_encode_passages(encoder ,passages):\n",
    "  batch_size=256\n",
    "  entity_name_features = []\n",
    "\n",
    "  num_passages = passages[\"input_ids\"].shape[0]\n",
    "\n",
    "  with torch.no_grad(): # Disable gradient calculation (saves tons of memory)\n",
    "    for i in tqdm(range(0, num_passages, batch_size), desc=\"Extracting features\"):\n",
    "\n",
    "        # extract entity features\n",
    "        # Autocast does the math in fp16 where possible (default is fp32)\n",
    "        # this will save memory and increase speed. The loss in precision shouldn't matter much (can check on a small sample if we want)\n",
    "        with autocast(\"cuda\"):\n",
    "          features = encoder(\n",
    "              input_ids=passages[\"input_ids\"][i:i + batch_size].to(device),\n",
    "              attention_mask=passages[\"attention_mask\"][i:i + batch_size].to(device)\n",
    "            ).representation\n",
    "\n",
    "          entity_name_features.append(features.detach().cpu()) # Detach and move to CPU to save VRAM/RAM\n",
    "\n",
    "\n",
    "  features_tensor = torch.cat(entity_name_features, dim=0)\n",
    "\n",
    "  return features_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "krqgyhIu46mL"
   },
   "source": [
    "# Q-nets take **a lot** of memory.\n",
    "\n",
    "Instead of creating all of them and then doing the similarity calculation, we will create batches and calculate similarities for just those q-nets, then discard those q-nets and move on to the next batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h-TaDiXwyiTD"
   },
   "outputs": [],
   "source": [
    "def batch_encode_queries(encoder, queries, passage_embeddings):\n",
    "  batch_size = 8\n",
    "  similarity_scores = []\n",
    "\n",
    "  num_queries = queries[\"input_ids\"].shape[0]\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for i in tqdm(range(0, num_queries, batch_size), desc=\"Creating q-nets and calculating similarity scores\"):\n",
    "\n",
    "        # create q-nets\n",
    "        with autocast(\"cuda\"):\n",
    "          q_nets = encoder(\n",
    "              input_ids=queries[\"input_ids\"][i:i + batch_size].to(device),\n",
    "              attention_mask=queries[\"attention_mask\"][i:i + batch_size].to(device)\n",
    "            ).representation\n",
    "\n",
    "\n",
    "        passages_gpu = passage_embeddings.to(device)\n",
    "\n",
    "        # Note: we use q_nets.num_queries (our repo's noTorch equivalent of q_nets.shape[0]) instead of batch_size\n",
    "        # because the total number might not be divisible by batch_size so the last batch might be smaller than the actual batch size\n",
    "        passages_batch = passages_gpu.unsqueeze(0).expand(q_nets.num_queries, -1, -1)\n",
    "\n",
    "        # calculate similarity\n",
    "        batch_scores = q_nets(passages_batch)\n",
    "        similarity_scores.append(batch_scores.detach().cpu())\n",
    "\n",
    "\n",
    "  scores_tensor = torch.cat(similarity_scores, dim=0)\n",
    "  return scores_tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pZo8sg5jKf5t"
   },
   "source": [
    "## Create a dataset of Hard Negatives based on \"Negative Hard Mining\"\n",
    "We take the top \"incorrect\" item similarities of each query as negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XLSHTrLnKf5t"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Desired format for each line in the JSONL file:\n",
    "{\n",
    "  \"query\": {\n",
    "    \"id\": query ID,\n",
    "    \"content\": query text,\n",
    "  },\n",
    "  \"items\": [\n",
    "    {\n",
    "      \"id\": passage ID,\n",
    "      \"content\": passage text,\n",
    "      \"score\": Optional teacher score,\n",
    "      \"type\": Sometimes used to specify type of item,\n",
    "    },\n",
    "    {\n",
    "        # another item\n",
    "    },\n",
    "  ]\n",
    "}\n",
    "\n",
    "Contrastive Loss with Hard Negatives: The positive must be the first item, all following items\n",
    "will be treated as negative\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9EiQt49cx-Pf"
   },
   "source": [
    "# Perform Hard Negative Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wn6bRxvvsMR5"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def write_hardneg_contrastive_jsonl_masked(\n",
    "    pairs,\n",
    "    similarity_scores: torch.Tensor,\n",
    "    output_jsonl_path: str,\n",
    "    num_negatives: int = 8,\n",
    "):\n",
    "\n",
    "    output_jsonl_path = Path(output_jsonl_path)\n",
    "    output_jsonl_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    similarity_scores = similarity_scores.detach().cpu()\n",
    "\n",
    "    similarity_scores = similarity_scores.squeeze(-1) # flatten last dimenstion\n",
    "    N, M = similarity_scores.shape\n",
    "\n",
    "    # Integer Mapping for Speed\n",
    "    # Convert string IDs to integers. unique_ids[inverse_indices[i]] == pairs[\"id\"][i]\n",
    "    # We use 'inverse_indices' to check for equality instantly (int vs int)\n",
    "    # np.unique returns 'inverse' which are the integer indices for each string\n",
    "    unique_ids, inverse_indices = np.unique(pairs[\"id\"], return_inverse=True)\n",
    "    id_tensor = torch.from_numpy(inverse_indices) # Shape: (N,)\n",
    "\n",
    "    # Compare every mention ID (row) against every entity ID (col)\n",
    "    # unsqueeze(1) makes it (N, 1)\n",
    "    # unsqueeze(0) makes it (1, M)\n",
    "    # The mask matches shape of similarity_scores (N, M)\n",
    "    mask = (id_tensor.unsqueeze(1) == id_tensor.unsqueeze(0))\n",
    "\n",
    "    # Apply Mask\n",
    "    # Set any cell where mask is True (same ID) to -infinity.\n",
    "    # This hides the query itself AND any other rows with the same ID.\n",
    "    similarity_scores.masked_fill_(mask, -float('inf'))\n",
    "\n",
    "    # Get Top-K Negatives\n",
    "    # Since positives are now -inf, topk will only return valid negatives.\n",
    "    _, top_idxs = torch.topk(similarity_scores, k=num_negatives, dim=1)\n",
    "\n",
    "\n",
    "    # Extract Negatives with Deduplication\n",
    "    with output_jsonl_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for i in range(N):\n",
    "            q_id = pairs[\"id\"][i]\n",
    "\n",
    "            # --- Dynamic Fetching Loop ---\n",
    "            # We need 'num_negatives' unique items.\n",
    "            # Since duplicates might clog the top of the list, we fetch more than we need.\n",
    "            # Start with a safe buffer (e.g., 3x what we need + 32).\n",
    "            k_attempt = (num_negatives * 4) + 32\n",
    "\n",
    "            valid_neg_indices = []\n",
    "            seen_entity_ids = set()\n",
    "\n",
    "            while len(valid_neg_indices) < num_negatives:\n",
    "                # Cap k at M (total entities)\n",
    "                if k_attempt > M:\n",
    "                    k_attempt = M\n",
    "\n",
    "                # Get top K candidates\n",
    "                # (Since positives are -inf, these are guaranteed to be negatives)\n",
    "                _, candidates = torch.topk(similarity_scores[i], k=k_attempt, dim=0, largest=True)\n",
    "\n",
    "                # Reset collection for this attempt with new k\n",
    "                valid_neg_indices = []\n",
    "                seen_entity_ids = set()\n",
    "\n",
    "                for idx in candidates.tolist():\n",
    "                    ent_int_id = inverse_indices[idx] # Get the integer ID of this candidate\n",
    "\n",
    "                    # Deduplicate: Have we seen this Entity ID in this negative list yet?\n",
    "                    if ent_int_id in seen_entity_ids:\n",
    "                        continue # Skip duplicate negative\n",
    "\n",
    "                    # Found a new unique negative\n",
    "                    seen_entity_ids.add(ent_int_id)\n",
    "                    valid_neg_indices.append(idx)\n",
    "\n",
    "                    if len(valid_neg_indices) >= num_negatives:\n",
    "                        break\n",
    "\n",
    "                # Check exit conditions\n",
    "                if len(valid_neg_indices) >= num_negatives:\n",
    "                    break # Success!\n",
    "                if k_attempt >= M:\n",
    "                    break # We searched the entire dataset\n",
    "\n",
    "                # Not enough unique negatives found? Double the search radius.\n",
    "                k_attempt *= 2\n",
    "\n",
    "            # --- Write to File ---\n",
    "            # Positive\n",
    "            pos_item = {\n",
    "                \"id\": pairs[\"id\"][i],\n",
    "                \"content\": pairs[\"entity\"][i],\n",
    "                \"score\": None, \"type\": None\n",
    "            }\n",
    "\n",
    "            # Negatives\n",
    "            neg_items = []\n",
    "            for idx in valid_neg_indices:\n",
    "                neg_items.append({\n",
    "                    \"id\": pairs[\"id\"][idx],\n",
    "                    \"content\": pairs[\"entity\"][idx],\n",
    "                    \"score\": None, \"type\": None\n",
    "                })\n",
    "\n",
    "            # Warn if dataset is too small/repetitive\n",
    "            if len(neg_items) < num_negatives:\n",
    "                 print(f\"Warning: Query {i} found only {len(neg_items)} unique negatives.\")\n",
    "\n",
    "            entry = {\n",
    "                \"query\": {\"id\": q_id, \"content\": pairs[\"mention\"][i]},\n",
    "                \"items\": [pos_item, *neg_items],\n",
    "            }\n",
    "            json.dump(entry, f, ensure_ascii=False)\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "    print(f\"Wrote {N} lines to {output_jsonl_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ewi8GeStL5yA"
   },
   "outputs": [],
   "source": [
    "# generate jsonl for contrastive loss for train/val/test splits\n",
    "\n",
    "data_splits = (\"train\", \"validation\", \"test\")\n",
    "seen = set()\n",
    "splits = [s for s in data_splits if (s in dataset and not (s in seen or seen.add(s)))]\n",
    "\n",
    "for split in splits:\n",
    "    print(f\"Starting {split} split\")\n",
    "    pairs = dataset[split]\n",
    "\n",
    "    # build query/passage lists for this split\n",
    "    queries = list(pairs[\"mention\"])\n",
    "    passages = list(pairs[\"entity\"])\n",
    "\n",
    "    # tokenize\n",
    "    query_inputs = tokenizer(queries, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    passage_inputs = tokenizer(passages, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "    # encode + score\n",
    "    passage_embeddings = batch_encode_passages(passage_encoder, passage_inputs)\n",
    "    similarity_scores = batch_encode_queries(query_encoder, query_inputs, passage_embeddings)\n",
    "\n",
    "    # write jsonl\n",
    "    write_hardneg_contrastive_jsonl_masked(\n",
    "        pairs=pairs,\n",
    "        similarity_scores=similarity_scores,\n",
    "        output_jsonl_path=f\"bc5cdr_{split}_hypencoder_contrastive.jsonl\",\n",
    "        num_negatives=8,\n",
    "    )\n",
    "\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L1aOKPJPP1w2"
   },
   "source": [
    "## Upload dataset to HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ElRo4grgf1R"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "\n",
    "data_files = {\n",
    "    \"train\": \"bc5cdr_train_hypencoder_contrastive.jsonl\",\n",
    "    \"validation\": \"bc5cdr_validation_hypencoder_contrastive.jsonl\",\n",
    "    \"test\": \"bc5cdr_test_hypencoder_contrastive.jsonl\",\n",
    "}\n",
    "\n",
    "# Load as a single DatasetDict\n",
    "ds = load_dataset(\"json\", data_files=data_files)\n",
    "\n",
    "# Push to Hub\n",
    "repo_id = \"Stevenf232/BC5CDR_nameonly_hard_negative_mining\"\n",
    "ds.push_to_hub(repo_id, private=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
