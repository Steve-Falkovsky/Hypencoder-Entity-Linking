{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"KgT3EnI0Pdpr","collapsed":true},"outputs":[],"source":["!pip install transformers tqdm more_itertools scikit-learn torch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p3BQUhKcGMVq"},"outputs":[],"source":["# Use this when working on the full entity dataset of 260_000 entities\n","# mention names aren't ordered inn a particular way, just what appears first in the documents\n","# we can have many duplicates, so in this case where we are only encoding the name we want to avoid that\n","# unique_mention_name_id_pairs = list({name: _id for name, _id in bc5cdr_name_id_pairs}.items())\n","# mention_names = unique_mention_name_id_pairs.keys()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tznThHxyRAON"},"outputs":[],"source":["# this is the feature extraction pipeline so we can get the embeddings directly (we can only do inference with this, no fine-tuning)\n","from transformers import pipeline\n","\n","model_name = \"cambridgeltl/SapBERT-from-PubMedBERT-fulltext\"\n","\n","# core model\n","extractor = pipeline(\"feature-extraction\", model=model_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NJxvpv2ck8cL"},"outputs":[],"source":["from datasets import load_dataset\n","\n","# there are all \"positive\" pairs\"\n","dataset = load_dataset(\"Stevenf232/BC5CDR_MeSH2015_nameonly\")\n","train_pairs = dataset[\"train\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lQZtEZub-Qas"},"outputs":[],"source":["from tqdm import tqdm\n","def extract_features(extractor ,pairs):\n","  batch_size=16\n","  # input all names to pipeline so it will create dense vectors of all the names (of the mentions and entities)\n","  mention_name_features = []\n","  entity_name_features = []\n","\n","  for i in tqdm(range(0, len(pairs), batch_size), desc=\"Extracting features\"):\n","      # extract mention features\n","      batch = pairs[i:i + batch_size][\"mention\"]\n","      features = extractor(batch, truncation=True, padding=True)\n","      mention_name_features.extend(features)\n","\n","      # extract entity features\n","      batch = pairs[i:i + batch_size][\"entity\"]\n","      features = extractor(batch, truncation=True, padding=True)\n","      entity_name_features.extend(features)\n","\n","  return mention_name_features, entity_name_features"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kGY4DeS5-YOq"},"outputs":[],"source":["mention_name_features, entity_name_features = extract_features(extractor, train_pairs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9o5MnQILUaAc"},"outputs":[],"source":["# ------------------ evaluating core model -------------------------\n","# In standard BERT and SapBERT models, the [CLS] (classification) token is located at the very beginning of the input sequence.\n","# once we have our features in shape [n,m,i]\n","# The CLS token which represents the word is in [0,0,:]\n","\n","import numpy as np\n","mention_vectors = [np.array(f[0][0]) for f in mention_name_features]\n","mention_vectors[0].shape\n","\n","entity_vectors = [np.array(f[0][0]) for f in entity_name_features]\n","entity_vectors[0].shape\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ixhc4C1ZEJD8"},"outputs":[],"source":["mention_vectors[0][:5]"]},{"cell_type":"markdown","metadata":{"id":"CGcetMJgmsjs"},"source":["# Simple model evaluation"]},{"cell_type":"markdown","metadata":{"id":"kKp8QG9r5B73"},"source":["Potential issue - this finds relevance using Cosine Similarity (will it have bias towards fine-tuning on cosineSimilarityLoss vs other loss functions?)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iVoiwedEGJ6P"},"outputs":[],"source":["from sklearn.metrics.pairwise import cosine_similarity\n","import numpy as np\n","\n","def evaluate(mention_vectors, entity_vectors):\n","  correct_count = 0\n","  for i, vector in enumerate(mention_vectors):\n","      scores = cosine_similarity(vector.reshape(1,-1), entity_vectors).flatten().tolist()\n","      top_idx = np.argmax(scores)\n","      # the strange conversion to int from here on out is because the original idx is of type numpy.int64\n","      top_match_id = train_pairs[int(top_idx)][\"id\"]\n","      correct_id = train_pairs[int(i)][\"id\"]\n","\n","      if top_match_id == correct_id:\n","          correct_count += 1\n","\n","      mention_name = train_pairs[int(i)][\"mention\"]\n","      top_match = train_pairs[int(top_idx)][\"entity\"]\n","      correct_name = train_pairs[int(i)][\"entity\"]\n","      print(f\"mention_name: {mention_name}\\ncorrect entity name: {correct_name}\\ntop_match: {top_match}\\n\")\n","\n","\n","  print(f\"total comparisons: {len(mention_vectors)}\")\n","  print(f\"correct comparisons: {correct_count}\")\n","  print(f\"accuracy: {correct_count / len(mention_vectors)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M57NXFT5PQgH"},"outputs":[],"source":["evaluate(mention_vectors, entity_vectors)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WkV4mUza7zPp"},"outputs":[],"source":["# more evaluation methods\n","from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n","# print(f\"{accuracy_score(train_labels, predicted_labels)=:.3f}\")\n","# print(f\"{recall_score(train_labels, predicted_labels)=:.3f}\")\n","# print(f\"{precision_score(train_labels, predicted_labels)=:.3f}\")\n","# print(f\"{f1_score(train_labels, predicted_labels)=:.3f}\")"]},{"cell_type":"markdown","metadata":{"id":"Tg8bVWw7m2fL"},"source":["## Evaluating Fine-tuned model\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XGh8CLlURCda"},"outputs":[],"source":["# finetuned model\n","# don't think I need a feature-extraction pipeline based on auto-generated docs: https://huggingface.co/Stevenf232/fine-tuned-SapBERT2\n","#from transformers import pipeline\n","\n","from sentence_transformers import SentenceTransformer\n","\n","fine_tuned_model_name = \"Stevenf232/fine-tuned-SapBERT4\"\n","model = SentenceTransformer(fine_tuned_model_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nNj9sQ-S-asd"},"outputs":[],"source":["from tqdm import tqdm\n","def encode(pairs):\n","  batch_size=16\n","  mention_encodings = []\n","  entity_encodings = []\n","\n","  for i in tqdm(range(0, len(pairs), batch_size), desc=\"Extracting features\"):\n","      # encode mentions\n","      batch = pairs[i:i + batch_size][\"mention\"]\n","      encodings = model.encode(batch)\n","      mention_encodings.extend(encodings)\n","\n","      # encode entities\n","      batch = pairs[i:i + batch_size][\"entity\"]\n","      encodings = model.encode(batch)\n","      entity_encodings.extend(encodings)\n","\n","  return mention_encodings, entity_encodings"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yJ6sDvTMTDqP"},"outputs":[],"source":["mention_encodings, entity_encodings = encode(train_pairs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ebo1BSGiYjJs"},"outputs":[],"source":["# ------------------ evaluating fine-tuned model -------------------------\n","# don't think I actually need to do this because we already have the pooling layer implemented as part of the sentece-transformer\n","# mention_vectors = [np.array(f[0][0]) for f in mention_name_features]\n","# entity_vectors = [np.array(f[0][0]) for f in entity_name_features]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G6t0GhSMD0w7"},"outputs":[],"source":["mention_vectors[0][:5]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xAs5NGaxPLVB"},"outputs":[],"source":["evaluate(mention_encodings, entity_encodings)"]}],"metadata":{"colab":{"gpuType":"T4","provenance":[],"private_outputs":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.5"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}