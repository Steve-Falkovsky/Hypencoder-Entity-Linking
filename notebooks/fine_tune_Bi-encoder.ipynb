{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"3bqyKk4ZI71b"},"outputs":[],"source":["# we have train, test, and validate data in 3 separate JSONL files which look like this:\n","{\"mention\": \"word1\", \"entity\": \"word2\", \"id\": \"unique_id_A\"}\n","{\"mention\": \"word3\", \"entity\": \"word4\", \"id\": \"unique_id_B\"}\n","{\"mention\": \"word5\", \"entity\": \"word6\", \"id\": \"unique_id_C\"}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zy1im1h7RCbH"},"outputs":[],"source":["%pip install datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2aCFsxxSfbKv"},"outputs":[],"source":["from google.colab import userdata\n","from huggingface_hub import login\n","\n","# Login into Hugging Face Hub\n","hf_token = userdata.get('HF_TOKEN') # If you are running inside a Google Colab\n","login(hf_token)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dsLS5uPoQxd3"},"outputs":[],"source":["from datasets import load_dataset\n","\n","# there are all \"positive\" pairs\"\n","dataset = load_dataset(\"Stevenf232/BC5CDR_MeSH2015_nameonly\")\n"]},{"cell_type":"markdown","metadata":{"id":"6MHK2qX1qMwF"},"source":["# Constructing negative pairs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"REEyV_z0jvos"},"outputs":[],"source":["# I want to have 4 times as many negative pairs as positive pairs\n","# There are several negative sampling techinques\n","# for now take one mention name from the positives and one entity name from the positives that don't match\n","# make it sample randomly from the positives\n","import random\n","\n","def create_negative_pairs(positive_pairs):\n","  negative_pairs = []\n","  while len(negative_pairs) < 4 * len(positive_pairs):\n","      entry = random.choice(positive_pairs)\n","      mention_name, mention_id = entry['mention'], entry['id']\n","      entry = random.choice(positive_pairs)\n","      entity_name, entity_id = entry['entity'], entry['id']\n","      if mention_id != entity_id:\n","          negative_pairs.append({\"mention\": mention_name, \"entity\": entity_name, \"id\": mention_id})\n","\n","  return negative_pairs\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aB8vLQJjlRf4"},"outputs":[],"source":["negative_pairs_test = create_negative_pairs(dataset['test'])\n","negative_pairs_train = create_negative_pairs(dataset['train'])\n","negative_pairs_validation = create_negative_pairs(dataset['validation'])"]},{"cell_type":"markdown","metadata":{"id":"HJ45131oqUZm"},"source":["# Combining Positive and Negative pairs, and adding labels\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VA2Z81yoj_X-"},"outputs":[],"source":["def add_labels(positive_pairs, negative_pairs):\n","  training_data = []\n","  for entry in positive_pairs:\n","      training_data.append({'mention': entry[\"mention\"], 'entity': entry[\"entity\"], 'label': 1}) # Changed to 1 (from True)\n","\n","  for entry in negative_pairs:\n","      training_data.append({'mention': entry[\"mention\"], 'entity': entry[\"entity\"], 'label': 0}) # Changed to 0 (from False)\n","\n","  return training_data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fgJT4GaJkNyT"},"outputs":[],"source":["train_data = add_labels(dataset['train'], negative_pairs_train)\n","test_data = add_labels(dataset['test'], negative_pairs_test)\n","validation_data = add_labels(dataset['validation'], negative_pairs_validation)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8xY25JEOnPmh"},"outputs":[],"source":["# load the new datasets\n","from datasets import Dataset, DatasetDict\n","\n","train_dataset = Dataset.from_list(train_data)\n","test_dataset = Dataset.from_list(test_data)\n","validation_dataset = Dataset.from_list(validation_data)\n","\n","dataset_dict = DatasetDict({\n","    'train': train_dataset,\n","    'test': test_dataset,\n","    'validation': validation_dataset,\n","})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qkGuD9WLdLjm"},"outputs":[],"source":["# sanity check entries from splits\n","# first come all the positive pairs, after that the negatives\n","print(\n","    dataset_dict[\"train\"][0],\n","    dataset_dict[\"test\"][0],\n","    dataset_dict[\"validation\"][-1]\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0bHWV56sePkb"},"outputs":[],"source":["train_pairs = dataset_dict['train']\n","val_pairs = dataset_dict['validation']"]},{"cell_type":"markdown","metadata":{"id":"Qtt07zdn30Ui"},"source":["# Training using sentence-transformers library (supported by HuggingFace)\n","To do fine tuning correctly, we need to treat this model as a sentence transformer (a.k.a Bi-encoder) because it's designed to compare whole sentences (e.g., to calculate similarity scores)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Scbfrkmc_gP"},"outputs":[],"source":["%pip install -U sentence-transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jZF3rRQU34di"},"outputs":[],"source":["from sentence_transformers import SentenceTransformer, models, losses\n","\n","model_name = 'cambridgeltl/SapBERT-from-PubMedBERT-fulltext'\n","\n","# 1. Load the base transformer model\n","word_embedding_model = models.Transformer(model_name)\n","\n","# 2. Add the correct pooling layer\n","# SapBERT uses the [CLS] token\n","pooling_model = models.Pooling(\n","    word_embedding_model.get_word_embedding_dimension(),\n","    pooling_mode='cls' # Use the first token (CLS token) as text representations\n",")\n","\n","# 3. Create the final SentenceTransformer model\n","model = SentenceTransformer(modules=[word_embedding_model, pooling_model])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XPKLgwQf5tWv"},"outputs":[],"source":["# Choosing Loss function\n","train_loss = losses.ContrastiveLoss(model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4KU6KNX5WeuZ"},"outputs":[],"source":["from sentence_transformers.trainer import SentenceTransformerTrainer\n","from sentence_transformers.training_args import SentenceTransformerTrainingArguments\n","\n","# Define training arguments\n","# using same config as in Jake's implementation\n","training_args = SentenceTransformerTrainingArguments(\n","    output_dir=\"models/my-finetuned-model\",\n","    num_train_epochs=3,\n","    learning_rate = 1e-5,\n","    optim = \"adamw_torch\",\n","    per_device_train_batch_size=8,\n","    save_strategy=\"epoch\",\n","    report_to=\"none\", # for some reason defaults to W&B (weights&biases), documentation states default is none\n",")\n","\n","# Create the Trainer\n","trainer = SentenceTransformerTrainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_pairs,\n","    eval_dataset=val_pairs,\n","    loss=train_loss,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yClI3tYBjHST"},"outputs":[],"source":["trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g53kf91WXquv"},"outputs":[],"source":["# create a new huggingFace repo for this model\n","model.push_to_hub(\"fine-tuned-SapBERT4\")\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"venv","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.12.5"}},"nbformat":4,"nbformat_minor":0}