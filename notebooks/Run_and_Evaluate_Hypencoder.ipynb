{"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/Steve-Falkovsky/Hypencoder-Entity-Linking/blob/main/notebooks/Hypencoder_Vs_fine_tuned_Hypencoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6fWDYcI90JIV"},"outputs":[],"source":["import os\n","\n","REPO_NAME = \"Hypencoder-Entity-Linking\"\n","GIT_URL = f\"https://github.com/Steve-Falkovsky/{REPO_NAME}.git\"\n","BRANCH_NAME = \"main\"\n","\n","!git clone -b {BRANCH_NAME} --single-branch {GIT_URL}\n","\n","# Move into the downloaded repo (The Root)\n","os.chdir(REPO_NAME)\n","\n","%pip install -q -e \"./hypencoder-paper\"\n","\n","os.chdir(\"hypencoder-paper\")\n","\n","print(f\"üìç Working Directory is now: {os.getcwd()}\")\n","print(\"‚úÖ Environment Ready!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IW5f-nK3rERc"},"outputs":[],"source":["from datasets import load_dataset\n","\n","# there are all \"positive\" pairs\n","dataset = load_dataset(\"Stevenf232/BC5CDR_MeSH2015_nameonly\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qPzJZy_YrWsJ"},"outputs":[],"source":["train_pairs = dataset['train']\n","print(train_pairs)\n","\n","mention_names = train_pairs['mention']\n","entity_names = train_pairs['entity']\n","print(mention_names[:3])\n","print(entity_names[:3])"]},{"cell_type":"markdown","metadata":{"id":"CF8-okEYP5m_"},"source":["### Load the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wN0mMKMd80bH"},"outputs":[],"source":["# Core Hypencoder model for outputing dense vector representations\n","from hypencoder_cb.modeling.hypencoder import Hypencoder, HypencoderDualEncoder, TextEncoder\n","from transformers import AutoTokenizer\n","\n","model_name = \"Stevenf232/hypencoder_BC5CDR\"\n","\n","dual_encoder = HypencoderDualEncoder.from_pretrained(model_name)\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","\n","\n","query_encoder: Hypencoder = dual_encoder.query_encoder\n","passage_encoder: TextEncoder = dual_encoder.passage_encoder"]},{"cell_type":"markdown","metadata":{"id":"spUPv5HAQBip"},"source":["### Move the model to the GPU"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LYjL82bYPt5r"},"outputs":[],"source":["import torch\n","\n","# Setup the device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")  # This should say 'cuda'\n","\n","# Move the model to the GPU\n","passage_encoder.to(device)\n","query_encoder.to(device)"]},{"cell_type":"markdown","metadata":{"id":"dLFqpGzvQQj_"},"source":["### Load datasets and tokenise"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pphGAan2ALIV"},"outputs":[],"source":["# convert from type \"datasets\" to python list\n","queries = list(mention_names)\n","passages = list(entity_names)\n","\n","\n","# the output of the tokenizer contains 3 fields:\n","# input_ids, token_type_ids, and attention_mask\n","# all contain a tensor in the shape (number of queries, max number of tokens)\n","\n","query_inputs = tokenizer(queries, return_tensors=\"pt\", padding=True, truncation=True)\n","passage_inputs = tokenizer(passages, return_tensors=\"pt\", padding=True, truncation=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QKBi8xau15hq"},"outputs":[],"source":["print(f\"query_inputs:\\n{query_inputs}\")\n","print(\"\\n\\n\\n\")\n","print(f\"passage_inputs:\\n{passage_inputs}\")"]},{"cell_type":"markdown","metadata":{"id":"qSxyVkRPBkCd"},"source":["# Passage Encodings\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wiCD611G0cu8"},"outputs":[],"source":["from tqdm import tqdm\n","import torch\n","from torch.amp import autocast\n","\n","def batch_encode_passages(encoder ,passages):\n","  batch_size=256\n","  entity_name_features = []\n","\n","  num_passages = passages[\"input_ids\"].shape[0]\n","\n","  with torch.no_grad(): # Disable gradient calculation (saves tons of memory)\n","    for i in tqdm(range(0, num_passages, batch_size), desc=\"Extracting features\"):\n","\n","        # extract entity features\n","        # Autocast does the math in fp16 where possible (default is fp32)\n","        # this will save memory and increase speed. The loss in precision shouldn't matter much (can check on a small sample if we want)\n","        with autocast(\"cuda\"):\n","          features = encoder(\n","              input_ids=passages[\"input_ids\"][i:i + batch_size].to(device),\n","              attention_mask=passages[\"attention_mask\"][i:i + batch_size].to(device)\n","            ).representation\n","\n","          entity_name_features.append(features.detach().cpu()) # Detach and move to CPU to save VRAM/RAM\n","\n","\n","  features_tensor = torch.cat(entity_name_features, dim=0)\n","\n","  return features_tensor"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XDO270MAN-P6"},"outputs":[],"source":["passage_embeddings = batch_encode_passages(passage_encoder, passage_inputs)"]},{"cell_type":"markdown","metadata":{"id":"ROQsRNS8WLUE"},"source":["##Now, we create the q-nets.\n","\n","For each q-net, we feed through it all the passages the calculate the similarity.\n","\n","But the q-nets are created in batches, and every batch is represented as a single object `NoTorchSequential`. (Check out the `RepeatedDenseBlockConverter` class in q_net.py for more info)\n","\n","This object expects an input in the shape (N, M, H):\n","\n","* N = number of queries (mentions)\n","\n","* M = number of passages (entities)\n","\n","* H = Hidden dimension (e.g., 768 for BERT)\n","\n","\n","\n","---\n","\n","\n","\n","The passage embeddings have the shape (M, H) so we must create an additional dimension of size N.\n","\n","This will be done like so:\n","`passages_batch = passages.unsqueeze(0).expand(num_queries, -1, -1)`\n","\n","* `.unsqueeze()` adds a new dimension (in our case at location 0)\n","\n","* `.expand()` \"expands\" that new dimension to be size \"num_queries\"\n","\n","* `.expand()` creates a view, so it costs almost 0 memory! (compared to .repeat() which changes the tensor)"]},{"cell_type":"markdown","metadata":{"id":"krqgyhIu46mL"},"source":["# Q-nets take **a lot** of memory.\n","\n","Instead of creating all of them and then doing the similarity calculation, we will create batches and calculate similarities for just those q-nets, then discard those q-nets and move on to the next batch."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h-TaDiXwyiTD"},"outputs":[],"source":["def batch_encode_queries(encoder, queries, passage_embeddings):\n","  batch_size = 8\n","  similarity_scores = []\n","\n","  num_queries = queries[\"input_ids\"].shape[0]\n","\n","  with torch.no_grad():\n","    for i in tqdm(range(0, num_queries, batch_size), desc=\"Creating q-nets and calculating similarity scores\"):\n","\n","        # create q-nets\n","        with autocast(\"cuda\"):\n","          q_nets = encoder(\n","              input_ids=queries[\"input_ids\"][i:i + batch_size].to(device),\n","              attention_mask=queries[\"attention_mask\"][i:i + batch_size].to(device)\n","            ).representation\n","\n","\n","        passages_gpu = passage_embeddings.to(device)\n","\n","        # Note: we use q_nets.num_queries (our repo's noTorch equivalent of q_nets.shape[0]) instead of batch_size\n","        # because the total number might not be divisible by batch_size so the last batch might be smaller than the actual batch size\n","        passages_batch = passages_gpu.unsqueeze(0).expand(q_nets.num_queries, -1, -1)\n","\n","        # calculate similarity\n","        batch_scores = q_nets(passages_batch)\n","        similarity_scores.append(batch_scores.detach().cpu())\n","\n","\n","  scores_tensor = torch.cat(similarity_scores, dim=0)\n","  return scores_tensor\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yTF_i9q5BlLc"},"outputs":[],"source":["similarity_scores = batch_encode_queries(query_encoder, query_inputs, passage_embeddings)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ml-zXxUfkjJz"},"outputs":[],"source":["# Case 1 - comparing a query to its respective passage\n","\n","# In the simple case where each q_net only takes one passage, we can just\n","# reshape the passage_embeddings to (N, 1, H).\n","# passage_embeddings_single = passage_embeddings.unsqueeze(1)\n","# print(f\"passage_embeddings shape: {passage_embeddings_single.shape}\")\n","# giving the nueral network the input of passage_embeddings\n","# the output provides the relevance score of query 1 against passage 1, query 2 against passage 2, etc...\n","# scores = q_nets(passage_embeddings_single)\n","# print(f\"scores: {scores}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tznThHxyRAON"},"outputs":[],"source":["# Case 2 - comparing a query to all passages\n","\n","# The case where each q_net takes multiple passages\n","# meaning multiple passages are now associated with each of the queries\n","\n","# this operation creates a 3D tensor which takes too much memory\n","# passage_embeddings_multi = passage_embeddings.repeat(N, 1).reshape(N, M, H)\n","# print(f\"passage_embeddings shape: {passage_embeddings_multi.shape}\")\n","\n","\n","# unbatched similarity scores for q-nets\n","# similarity_scores = q_nets(passage_embeddings_multi)\n","# print(f\"similarity_scores shape: {similarity_scores.shape}\")\n","#print(f\"similarity_scores: {similarity_scores}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EIdGW0H4bsC7"},"outputs":[],"source":["similarity_scores"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a5fJF7iVBLHz"},"outputs":[],"source":["from sklearn.metrics.pairwise import cosine_similarity\n","import numpy as np\n","\n","def evaluate(train_pairs):\n","  correct_count = 0\n","  top_idxs = torch.argmax(similarity_scores,dim=1).flatten()\n","\n","  for i in range(len(queries)):\n","      top_idx = top_idxs[i]\n","      # the conversion to int from here on out is because the original idx is of type numpy.int64\n","      top_match_id = train_pairs[\"id\"][int(top_idx)]\n","      correct_id = train_pairs[\"id\"][int(i)]\n","\n","      if top_match_id == correct_id:\n","          correct_count += 1\n","\n","      mention_name = train_pairs[\"mention\"][int(i)]\n","      top_match = train_pairs[\"entity\"][int(top_idx)]\n","      correct_name = train_pairs[\"entity\"][int(i)]\n","      print(f\"mention_name: {mention_name}\\ncorrect entity name: {correct_name}\\ntop_match: {top_match}\\n\")\n","\n","\n","  print(f\"total comparisons: {len(queries)}\")\n","  print(f\"correct comparisons: {correct_count}\")\n","  print(f\"accuracy: {correct_count / len(queries)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q5MDujF5BMbb"},"outputs":[],"source":["evaluate(train_pairs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WkV4mUza7zPp"},"outputs":[],"source":["# more evaluation methods\n","from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n","# print(f\"{accuracy_score(train_labels, predicted_labels)=:.3f}\")\n","# print(f\"{recall_score(train_labels, predicted_labels)=:.3f}\")\n","# print(f\"{precision_score(train_labels, predicted_labels)=:.3f}\")\n","# print(f\"{f1_score(train_labels, predicted_labels)=:.3f}\")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"venv","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.12.5"}},"nbformat":4,"nbformat_minor":0}