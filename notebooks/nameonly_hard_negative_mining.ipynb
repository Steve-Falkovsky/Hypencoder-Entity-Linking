{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Steve-Falkovsky/Hypencoder-Entity-Linking/blob/main/notebooks/Hypencoder_Vs_fine_tuned_Hypencoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6fWDYcI90JIV",
    "outputId": "1ea70c28-5ca5-4636-94af-8af3ccaee8ce"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import importlib.util\n",
    "\n",
    "REPO_NAME = \"Hypencoder-Entity-Linking\"\n",
    "GIT_URL = f\"https://github.com/Steve-Falkovsky/{REPO_NAME}.git\"\n",
    "BRANCH_NAME = \"Professional-Structure\"\n",
    "\n",
    "if not os.path.exists(REPO_NAME):\n",
    "    !git clone -b {BRANCH_NAME} --single-branch {GIT_URL}\n",
    "\n",
    "    # Move into the downloaded repo (The Root)\n",
    "    os.chdir(REPO_NAME)\n",
    "\n",
    "\n",
    "%pip install -q -e \"./hypencoder-paper\"\n",
    "\n",
    "os.chdir(\"hypencoder-paper\")\n",
    "\n",
    "print(f\"ðŸ“ Working Directory is now: {os.getcwd()}\")\n",
    "print(\"âœ… Environment Ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241,
     "referenced_widgets": [
      "1ab74cabe9c8412cbe41495b81756a50",
      "0907b6b60c0d4bdf98c5b35f79f87a2f",
      "c02ee862ca7043ce95843f573b0d7d31",
      "c57575e43137466994ef5518e4d52ddc",
      "775d84d92a404a5ebcf663f467284fc2",
      "22559862e532465cb7ea6d198bf860f5",
      "0012140e37eb4857ba5a3add6a03e7e1",
      "e6399955ea3a4e8bb1594fca052a5923",
      "d7d1c370cc2b4a9a894235edf12a4df3",
      "1cc1139a252c482c9b6790d498317f1b",
      "20859695c4404782b3055d26b7cdea23",
      "a1cf548cdf014656bd7e510c8148e2dc",
      "bf86f68718a141618b229886cb42255b",
      "5a4c91f7cb5241c396c1b6865fe80ff4",
      "37195e7fce6f4651b3a4b62234fa7a9c",
      "16a443f3f9e948acb1532e82752c5afa",
      "7f8da3d36e464c9f9dcd8bde99dd7f5f",
      "d22ca5335a5b41e79ffb20c7387afe95",
      "28b807a8f43a41b380d2598e9d0058e6",
      "1e214c2b77a540dbbda487c8cc7b8f55",
      "7bdd7e80a9e0413b87d62731e2937562",
      "943a29b626ae4558a14261a0e9285c7c",
      "7b0004f81399496cab747f3ec7d8981c",
      "4a38c2e67ac949ce9390dfcd56b300a5",
      "4879f37f9768478d88b1642428172933",
      "d10e367bdde6414ca3b2299dd00197fb",
      "0b1fa17cb9a94106b6b652dd65cc2b82",
      "f639207ad66040db9e740b05d8d1dced",
      "1adf78e7596a48e4846a105466a13131",
      "ea2f52e67ac5489bbd8a86e5106d7f8d",
      "5c518cfb55804242891c422239fc0bc9",
      "71aafecad89043b88299367fcb3c7ed4",
      "9e64f595cb0548f0a671e29572ddefa2",
      "66a91a98622d4f0ba8781da08db255db",
      "915007551b624357a042f015377751d8",
      "903eeaefcb4543a2994f3b27828b0521",
      "995802d2205f4f03a659f21c8185cd4d",
      "46d962cdd0034dea8c6aa1f119fee733",
      "bd0ec40641774156a43b1a0cc9bf701e",
      "1816bea90eed4eb9a59b195c134568ef",
      "59f6af0712fb4eaa8946f7d62041c693",
      "939886df87054151b1bf2a798eff7866",
      "7ee56250c0264ca192a6e321e861b2ad",
      "0592e5b3e81342d98bf6791eb855147b",
      "ddd86a3cf17c4232be9d78011bc5ab51",
      "c05bb06e23194e0a84796abbbce95cbf",
      "1ea3067b740342b596acf252a3ed72a5",
      "f78adb4d621d46ad9d6b380ba8a7c5e2",
      "6dcd6daa25a64f50a23237593ab76b15",
      "9c06909fa6ac43e6ad8ee9be747479da",
      "a9d0fc52d63848458f0317aa0e910fce",
      "0922758262c44b92a30e6177d01ab338",
      "340901c1d71c469f9c42e839db94b8f5",
      "1c3bdc4d4b6a42f285d81b7115bd72c1",
      "17135642d87a4d8fa21afa6d90b10ad7",
      "a9cab414f05246b5a72668c3f13bec04",
      "1edfb81fbaa8496ea5ab04f682a33175",
      "6c91732ca7074417a7eeb0a69cb6f27d",
      "50a69577631542699093763456155a67",
      "223dc6028e084a339f920be568bf8a71",
      "20843be48a6c4547bf27ced159a2da7d",
      "852d827b643e4156b9165845500cafe3",
      "944495b76f41465eb6f82fb837dc1071",
      "0d32fa444ece42118be18ef6ca102d4e",
      "bb4387e65c7e4caa93766bf52c3c9767",
      "ab013dc7c8b94a20a7896dd6860590ac",
      "cfd0201c59aa449abecbac642be2c3cb",
      "68cd55e28c124bc1bdc6bd40b2d65198",
      "98b9a5fe9e4a4473a9c8955785490500",
      "7042e849888b4a62ae1b9f0e6a5b943c",
      "ef86c9db3e354e0eb32c8c8f7f7551cd",
      "a1e42313458e4585bfd8db8c4b2eaf86",
      "6da1bab4d3154ce0b7628f60ee06a4a3",
      "5e54a04b312a4b4e8eeb500c96c4c6e8",
      "9b100b739b424c0a89676af35756e4f1",
      "179535e822fc4fc2b0a60b1ade03bb67",
      "ea446e8404a4475d85936d40e295ee35"
     ]
    },
    "id": "IW5f-nK3rERc",
    "outputId": "9f6ce53f-5ff3-4ca8-ccb9-20b5157e4aad"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# there are all \"positive\" pairs\"\n",
    "dataset = load_dataset(\"Stevenf232/BC5CDR_MeSH2015_nameonly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qPzJZy_YrWsJ",
    "outputId": "bbb4ebaa-8e53-47e9-e151-6dc6ad04e72c"
   },
   "outputs": [],
   "source": [
    "train_pairs = dataset['train']\n",
    "print(train_pairs)\n",
    "\n",
    "mention_names = train_pairs['mention']\n",
    "entity_names = train_pairs['entity']\n",
    "print(mention_names[:3])\n",
    "print(entity_names[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CF8-okEYP5m_"
   },
   "source": [
    "### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241,
     "referenced_widgets": [
      "93b9722290cf4e3080b999a52a26790d",
      "d1b84254d1524cf18b442a6fca24141b",
      "f16bdb2e83104e3486926af640d2255e",
      "3dc516f0430947839863adc2519cd82e",
      "8d4d1042c7814bb3ae7825794a08dc23",
      "0eaac48ba47b476d8075d38a1bb7d237",
      "ae3b71f1a3cd4876b553f4956f435411",
      "f113cd4d648f48b2bdafa74b1a2f0f3e",
      "85b83e78bf6a4ffba0425b6b7dc3be8a",
      "a8adcf60671a472cb762995bed01d51c",
      "6d0684d265534e4590a8daeff1a5114b",
      "e3d62c6b665242548ac8f56e1df218d3",
      "72bec9d3a0cd425e9b58cb89f93ea816",
      "4bd8d31e341a4ba48b582715cd9fecef",
      "590d52234a6149719925d48764c358e2",
      "de6c93bef57c4eeaae89eedd03b541b2",
      "c1b1bc962a154607adb130ca6ac59648",
      "e8ab5454beda46cc8314a2b67cd2fd65",
      "af82e6c12ca941958283015101f62a7f",
      "6424c2125aec451c9408f3da7c998eae",
      "40eb7c273dde49ef936f43ac0ee70a8f",
      "03253b8a8c0e4f15acba0b22a4dbd530",
      "d30700ca6f7942159a6074c8e8267a18",
      "84c8abd0e77b42e2971e901eec0c4f7e",
      "cf803fdbd9194d13b53118acf0624611",
      "fe0a4694c64645b0bc2ab2caaf11331e",
      "ef674db715be49f1bc0503dbc11cdb7e",
      "a19ef90c00f248fe8c9d37c7c3fad8e5",
      "a93f4ed33ba045a6b527e6d2091721c1",
      "94590bf50dcb439c844a8423ae5415f9",
      "6c5879d29ea94482b356956439f8abce",
      "558722ea0fbd459aa30a876e3e88795b",
      "3b1c838fc1694747b411b482524877a0",
      "61848a0ebb984b778f5c8b244ca7cc07",
      "d664d9dfbbeb4682af67da90c4fec87f",
      "0607ef3627b84d92a8c5833bd69a3456",
      "c08f6bfd08b14fbbb71f0dd51f139798",
      "78af3b0f551e423a864416998e5e203f",
      "80b52b25210c411aa36608f120d0d8e0",
      "ecb8774ecfff4347bffa0f6863276cf1",
      "1bbe18446a1e41e9bcf2bc35388f45eb",
      "4cc76b245f1440fdaa370fd54b892f35",
      "39e5491265864b50bd7dd2abe1e47d9a",
      "483b8458bd054b79a5218b285d5f1f17",
      "07fc5b3a2b68483bba7a8b120b18d670",
      "d14e8b4ad5f64c32b92cc190553371c2",
      "ca77ee7723534c94abbd3b4c7d37e7da",
      "8d8666a81ba848349e2dc555498be4ce",
      "10aedfd96429445dbd2ae60ecf971bf2",
      "9eef9b5f299d469c81b0f7aeb8519068",
      "f12b311ee92f40d993803e04aff4d039",
      "a9a8e54110374039aa69f9929318e0ba",
      "c1335de309c24e40bbc14f4ea73dc10f",
      "ce6969202c804a9abab981f1b6426da1",
      "8860a290d4b24513906f4822808bf275",
      "60d19400320f4cd38bc2ccce73f2d651",
      "5d6ea06253c04896823394038621908d",
      "8b96d90dd8f3489bb0db62f0190dc152",
      "b7538e5f21d04edb8df145f761ff3c46",
      "9c09bf3c9d604bcd9ac8fcf0af9686e8",
      "afc8a0227e6c4c17a7840f21eb3763d9",
      "00afef6d121b4290b71e1c3960c2b4f6",
      "a18544b3a4db49c1ad440a19cc2c4831",
      "c964b669cd7c49b5bd7730b3de4ab21e",
      "d44d630ac74d4f7093822850b24be0d6",
      "1e24f144c3e94e4fae28d7e042159311",
      "c18bb645ca71456598adb719e3411fae",
      "0bfd9ab4c7c143ebb525cfe7aa3a3264",
      "f2862b5dd5c14f0592effa8fc657dea6",
      "01b0fa7151894472815c69e52dd70702",
      "e760e6ee67a746d79cff99c76b579c8d",
      "d5025bc7fac14706969f6bc8470e84e8",
      "97c89c2e5aae4212ae9e78645ba4e784",
      "01281c588253444586ab00ed858ada4d",
      "a94fe9e6b1bc4e06b81e642a4ec3954c",
      "c0e9bc11fb274a2c94dfd9bc5db35fea",
      "e3ebab5f4f1f419390494b0a49b8c9fb"
     ]
    },
    "id": "wN0mMKMd80bH",
    "outputId": "13d95c12-2438-400c-8cd5-3a5417254e28"
   },
   "outputs": [],
   "source": [
    "# Core Hypencoder model for outputing dense vector representations\n",
    "from hypencoder_cb.modeling.hypencoder import Hypencoder, HypencoderDualEncoder, TextEncoder\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"Stevenf232/SapBERT_freeze_hypencoder\"\n",
    "\n",
    "dual_encoder = HypencoderDualEncoder.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "query_encoder: Hypencoder = dual_encoder.query_encoder\n",
    "passage_encoder: TextEncoder = dual_encoder.passage_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "spUPv5HAQBip"
   },
   "source": [
    "### Move the model to the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LYjL82bYPt5r"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Setup the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")  # This should say 'cuda'\n",
    "\n",
    "# Move the model to the GPU\n",
    "passage_encoder.to(device)\n",
    "query_encoder.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dLFqpGzvQQj_"
   },
   "source": [
    "### Load datasets and tokenise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pphGAan2ALIV"
   },
   "outputs": [],
   "source": [
    "# convert from type \"datasets\" to python list\n",
    "queries = list(mention_names)\n",
    "passages = list(entity_names)\n",
    "\n",
    "\n",
    "# the output of the tokenizer contains 3 fields:\n",
    "# input_ids, token_type_ids, and attention_mask\n",
    "# all contain a tensor in the shape (number of queries, max number of tokens)\n",
    "\n",
    "query_inputs = tokenizer(queries, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "passage_inputs = tokenizer(passages, return_tensors=\"pt\", padding=True, truncation=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QKBi8xau15hq",
    "outputId": "336cf50c-f5de-475d-cb04-d0d9962ba50f"
   },
   "outputs": [],
   "source": [
    "print(f\"query_inputs:\\n{query_inputs}\")\n",
    "print(\"\\n\\n\\n\")\n",
    "print(f\"passage_inputs:\\n{passage_inputs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qSxyVkRPBkCd"
   },
   "source": [
    "# Passage Encodings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wiCD611G0cu8"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.amp import autocast\n",
    "\n",
    "def batch_encode_passages(encoder ,passages):\n",
    "  batch_size=256\n",
    "  entity_name_features = []\n",
    "\n",
    "  num_passages = passages[\"input_ids\"].shape[0]\n",
    "\n",
    "  with torch.no_grad(): # Disable gradient calculation (saves tons of memory)\n",
    "    for i in tqdm(range(0, num_passages, batch_size), desc=\"Extracting features\"):\n",
    "\n",
    "        # extract entity features\n",
    "        # Autocast does the math in fp16 where possible (default is fp32)\n",
    "        # this will save memory and increase speed. The loss in precision shouldn't matter much (can check on a small sample if we want)\n",
    "        with autocast(\"cuda\"):\n",
    "          features = encoder(\n",
    "              input_ids=passages[\"input_ids\"][i:i + batch_size].to(device),\n",
    "              attention_mask=passages[\"attention_mask\"][i:i + batch_size].to(device)\n",
    "            ).representation\n",
    "\n",
    "          entity_name_features.append(features.detach().cpu()) # Detach and move to CPU to save VRAM/RAM\n",
    "\n",
    "\n",
    "  features_tensor = torch.cat(entity_name_features, dim=0)\n",
    "\n",
    "  return features_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XDO270MAN-P6",
    "outputId": "06e590e7-5545-42ae-9aa5-fa49a4809813"
   },
   "outputs": [],
   "source": [
    "passage_embeddings = batch_encode_passages(passage_encoder, passage_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ROQsRNS8WLUE"
   },
   "source": [
    "## Now, we create the q-nets.\n",
    "\n",
    "For each q-net, we feed through it all the passages the calculate the similarity.\n",
    "\n",
    "But the q-nets are created in batches, and every batch is represented as a single object `NoTorchSequential`. (Check out the `RepeatedDenseBlockConverter` class in q_net.py for more info)\n",
    "\n",
    "This object expects an input in the shape (N, M, H):\n",
    "\n",
    "* N = number of queries (mentions)\n",
    "\n",
    "* M = number of passages (entities)\n",
    "\n",
    "* H = Hidden dimension (e.g., 768 for BERT)\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "The passage embeddings have the shape (M, H) so we must create an additional dimension of size N.\n",
    "\n",
    "This will be done like so:\n",
    "`passages_batch = passages.unsqueeze(0).expand(num_queries, -1, -1)`\n",
    "\n",
    "* `.unsqueeze()` adds a new dimension (in our case at location 0)\n",
    "\n",
    "* `.expand()` \"expands\" that new dimension to be size \"num_queries\"\n",
    "\n",
    "* `.expand()` creates a view, so it costs almost 0 memory! (compared to .repeat() which changes the tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "krqgyhIu46mL"
   },
   "source": [
    "# Q-nets take **a lot** of memory.\n",
    "\n",
    "Instead of creating all of them and then doing the similarity calculation, we will create batches and calculate similarities for just those q-nets, then discard those q-nets and move on to the next batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h-TaDiXwyiTD"
   },
   "outputs": [],
   "source": [
    "def batch_encode_queries(encoder, queries, passage_embeddings):\n",
    "  batch_size = 8\n",
    "  similarity_scores = []\n",
    "\n",
    "  num_queries = queries[\"input_ids\"].shape[0]\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for i in tqdm(range(0, num_queries, batch_size), desc=\"Creating q-nets and calculating similarity scores\"):\n",
    "\n",
    "        # create q-nets\n",
    "        with autocast(\"cuda\"):\n",
    "          q_nets = encoder(\n",
    "              input_ids=queries[\"input_ids\"][i:i + batch_size].to(device),\n",
    "              attention_mask=queries[\"attention_mask\"][i:i + batch_size].to(device)\n",
    "            ).representation\n",
    "\n",
    "\n",
    "        passages_gpu = passage_embeddings.to(device)\n",
    "\n",
    "        # Note: we use q_nets.num_queries (our repo's noTorch equivalent of q_nets.shape[0]) instead of batch_size\n",
    "        # because the total number might not be divisible by batch_size so the last batch might be smaller than the actual batch size\n",
    "        passages_batch = passages_gpu.unsqueeze(0).expand(q_nets.num_queries, -1, -1)\n",
    "\n",
    "        # calculate similarity\n",
    "        batch_scores = q_nets(passages_batch)\n",
    "        similarity_scores.append(batch_scores.detach().cpu())\n",
    "\n",
    "\n",
    "  scores_tensor = torch.cat(similarity_scores, dim=0)\n",
    "  return scores_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yTF_i9q5BlLc",
    "outputId": "10e26667-82b4-4b83-b741-6d39b4467a68"
   },
   "outputs": [],
   "source": [
    "similarity_scores = batch_encode_queries(query_encoder, query_inputs, passage_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ml-zXxUfkjJz"
   },
   "outputs": [],
   "source": [
    "# Case 1 - comparing a query to its respective passage\n",
    "\n",
    "# In the simple case where each q_net only takes one passage, we can just\n",
    "# reshape the passage_embeddings to (N, 1, H).\n",
    "# passage_embeddings_single = passage_embeddings.unsqueeze(1)\n",
    "# print(f\"passage_embeddings shape: {passage_embeddings_single.shape}\")\n",
    "# giving the nueral network the input of passage_embeddings\n",
    "# the output provides the relevance score of query 1 against passage 1, query 2 against passage 2, etc...\n",
    "# scores = q_nets(passage_embeddings_single)\n",
    "# print(f\"scores: {scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tznThHxyRAON"
   },
   "outputs": [],
   "source": [
    "# Case 2 - comparing a query to all passages\n",
    "\n",
    "# The case where each q_net takes multiple passages\n",
    "# meaning multiple passages are now associated with each of the queries\n",
    "\n",
    "# this operation creates a 3D tensor which takes too much memory\n",
    "# passage_embeddings_multi = passage_embeddings.repeat(N, 1).reshape(N, M, H)\n",
    "# print(f\"passage_embeddings shape: {passage_embeddings_multi.shape}\")\n",
    "\n",
    "\n",
    "# unbatched similarity scores for q-nets\n",
    "# similarity_scores = q_nets(passage_embeddings_multi)\n",
    "# print(f\"similarity_scores shape: {similarity_scores.shape}\")\n",
    "#print(f\"similarity_scores: {similarity_scores}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EIdGW0H4bsC7",
    "outputId": "8a7c8a16-229f-4be4-c4a1-6f1d81ed184f"
   },
   "outputs": [],
   "source": [
    "similarity_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a dataset of Hard Negatives based on \"Negative Hard Mining\"\n",
    "We take the top \"incorrect\" item similarities of each query as negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Desired format for each line in the JSONL file:\n",
    "{\n",
    "  \"query\": {\n",
    "    \"id\": query ID,\n",
    "    \"content\": query text,\n",
    "  },\n",
    "  \"items\": [\n",
    "    {\n",
    "      \"id\": passage ID,\n",
    "      \"content\": passage text,\n",
    "      \"score\": Optional teacher score,\n",
    "      \"type\": Sometimes used to specify type of item,\n",
    "    },\n",
    "    {\n",
    "        # another item\n",
    "    },\n",
    "  ]\n",
    "}\n",
    "\n",
    "Contrastive Loss with Hard Negatives: The positive must be the first item, all following items\n",
    "will be treated as negative\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import torch\n",
    "\n",
    "def write_hardneg_contrastive_jsonl(\n",
    "    pairs,                           \n",
    "    similarity_scores: torch.Tensor,\n",
    "    output_jsonl_path: str | Path,\n",
    "    num_negatives: int = 8,\n",
    "):\n",
    "    \"\"\"\n",
    "    For each query:\n",
    "      - items[0] = the correct entity\n",
    "      - items[1:1+num_negatives] = top scoring incorrect entities\n",
    "    \"\"\"\n",
    "    \n",
    "    output_jsonl_path = Path(output_jsonl_path)\n",
    "    output_jsonl_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Ensure CPU tensor for safe indexing\n",
    "    similarity_scores = similarity_scores.detach().cpu()\n",
    "\n",
    "\n",
    "    # check shapes are correct\n",
    "    N, M = similarity_scores.shape\n",
    "    if len(pairs[\"mention\"]) != N:\n",
    "        raise ValueError(f\"Mismatch: scores have N={N}, but pairs has {len(pairs['mention'])} mentions\")\n",
    "    if len(pairs[\"entity\"]) != M:\n",
    "        raise ValueError(f\"Mismatch: scores have M={M}, but pairs has {len(pairs['entity'])} entities\")\n",
    "\n",
    "\n",
    "    # generate the jsonl entires\n",
    "    with output_jsonl_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for i in range(N):\n",
    "            q_id = pairs[\"id\"][i]\n",
    "            mention = pairs[\"mention\"][i]\n",
    "\n",
    "            # Positive\n",
    "            pos_item = {\n",
    "                \"id\": pairs[\"id\"][i],\n",
    "                \"content\": pairs[\"entity\"][i],\n",
    "                \"score\": None,\n",
    "                \"type\": None,\n",
    "            }\n",
    "\n",
    "            # Get top candidates, then filter out the true positive index\n",
    "            _, top_idxs = torch.topk(similarity_scores[i], k=num_negatives+1, largest=True, sorted=True)\n",
    "\n",
    "            neg_items = []\n",
    "            for j in top_idxs.tolist():\n",
    "                if j == i:\n",
    "                    continue\n",
    "                neg_items.append(\n",
    "                    {\n",
    "                        \"id\": pairs[\"id\"][j],\n",
    "                        \"content\": pairs[\"entity\"][j],\n",
    "                        \"score\": None,\n",
    "                        \"type\": None,\n",
    "                    }\n",
    "                )\n",
    "                if len(neg_items) >= num_negatives:\n",
    "                    break\n",
    "\n",
    "            if len(neg_items) < num_negatives:\n",
    "                raise RuntimeError(f\"Only collected {len(neg_items)} negatives for query {i} (M={M}).\")\n",
    "\n",
    "            entry = {\n",
    "                \"query\": {\"id\": q_id, \"content\": mention},\n",
    "                \"items\": [pos_item, *neg_items],\n",
    "            }\n",
    "            json.dump(entry, f, ensure_ascii=False)\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "    print(f\"Wrote {N} lines to {output_jsonl_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate jsonl for contrastiev loss for train split\n",
    "write_hardneg_contrastive_jsonl(\n",
    "    pairs=train_pairs,\n",
    "    similarity_scores=similarity_scores,\n",
    "    output_jsonl_path=\"bc5cdr_train_hypencoder_contrastive.jsonl\",\n",
    "    num_negatives=8,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
